{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Retrieval-Augmented Generation (RAG) with Local Llama 2 & ChromaDB\n",
    "\n",
    "## Overview\n",
    "This notebook implements an **Agentic Retrieval-Augmented Generation (RAG) pipeline** using a local **Llama 2** and **ChromaDB** for intelligent question-answering. The system determines whether additional context is needed before generating responses, ensuring high accuracy.\n",
    "\n",
    "### Key Features:\n",
    "- **Llama 2 Model** for high-quality text generation.\n",
    "- **PDF Document Processing** to extract relevant information.\n",
    "- **ChromaDB Vector Store** for efficient semantic search.\n",
    "- **Dynamic Context Retrieval** to improve answer accuracy.\n",
    "- **Two Answering Modes**:\n",
    "  - With RAG (Retrieves relevant document content before responding).\n",
    "  - Without RAG (Directly generates responses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if you have not installed them already\n",
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Hugging Face Hub for Model Download\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# LangChain Components\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Sentence Transformers for Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ChromaDB for Vector Storage\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Transformers \n",
    "import transformers\n",
    "import sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 1: Model Setup\n",
    "\n",
    "We will set up **Llama 2 (7B)** for text generation. If the model is not found locally, it will be downloaded from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at: model/llama-2-7b-chat.Q4_K_M.gguf\n",
      "Using model at: model/llama-2-7b-chat.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILENAME = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "MODEL_DIR = \"model\"\n",
    "EXPECTED_PATH = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "\n",
    "# Ensure model directory exists\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Check if model already exists\n",
    "if os.path.exists(EXPECTED_PATH):\n",
    "    print(f\"Model already exists at: {EXPECTED_PATH}\")\n",
    "    model_path = EXPECTED_PATH\n",
    "else:\n",
    "    print(\"Model not found locally. Downloading Llama 2 model...\")\n",
    "    \n",
    "    # Download the model\n",
    "    model_path = hf_hub_download(\n",
    "        repo_id=\"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
    "        filename=MODEL_FILENAME,\n",
    "        local_dir=MODEL_DIR\n",
    "    )\n",
    "    print(f\"Model downloaded to: {model_path}\")\n",
    "\n",
    "print(f\"Using model at: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with the local path and GPU acceleration\n",
    "llm = LlamaCpp(\n",
    "    model_path=EXPECTED_PATH,\n",
    "    temperature=0.25,\n",
    "    max_tokens=2000,\n",
    "    n_ctx=4096,\n",
    "    top_p=1.0,\n",
    "    verbose=False,\n",
    "    n_gpu_layers=30,  # Utilize some available GPU layers\n",
    "    n_batch=512,      # Optimize batch size for parallel processing\n",
    "    f16_kv=True,      # Enable half-precision for key/value cache\n",
    "    use_mlock=True,   # Lock memory to prevent swapping\n",
    "    use_mmap=True     # Utilize memory mapping for faster loading\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ Step 2: Loading and Processing the PDF Document\n",
    "\n",
    "To enable context-aware question-answering, we load a **PDF document**, extract its content, and split it into manageable chunks for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF from: ./data/AIStudioDoc.pdf\n",
      "Successfully loaded 8 document(s) from the PDF.\n"
     ]
    }
   ],
   "source": [
    "# --- Load the PDF Document ---\n",
    "\n",
    "# Define the PDF file path\n",
    "PDF_PATH = \"./data/AIStudioDoc.pdf\"\n",
    "print(f\"Loading PDF from: {PDF_PATH}\")\n",
    "\n",
    "# Load the PDF document\n",
    "pdf_loader = PyPDFLoader(PDF_PATH)\n",
    "documents = pdf_loader.load()\n",
    "\n",
    "print(f\"Successfully loaded {len(documents)} document(s) from the PDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Step 3: Splitting the Document into Chunks\n",
    "\n",
    "Since large documents are difficult to process in full, we split the text into **small overlapping chunks** of approximately **500 characters**. These chunks will later be embedded and stored in ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully split PDF into 8 text chunks.\n"
     ]
    }
   ],
   "source": [
    "# --- Split the PDF Content into Manageable Chunks ---\n",
    "\n",
    "# Define text splitting parameters\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "# Split the PDF content into chunks\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Successfully split PDF into {len(docs)} text chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 4: Initializing the Embedding Model\n",
    "\n",
    "To convert text into numerical representations for efficient similarity search, we use **all-MiniLM-L6-v2** from `sentence-transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded embedding model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize the Embedding Model ---\n",
    "\n",
    "# Define the embedding model name\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "print(f\"Successfully loaded embedding model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Step 5: Computing Embeddings for Document Chunks\n",
    "\n",
    "Each chunk is converted into a **vector representation** using our embedding model. This allows us to perform **semantic similarity searches** later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully computed embeddings for each text chunk.\n",
      "Embeddings Shape: (8, 384)\n"
     ]
    }
   ],
   "source": [
    "# --- Compute Embeddings for Each Text Chunk ---\n",
    "\n",
    "# Extract text content from each chunk\n",
    "doc_texts = [doc.page_content for doc in docs]\n",
    "\n",
    "# Compute embeddings for the extracted text chunks\n",
    "document_embeddings = embedding_model.encode(doc_texts, convert_to_numpy=True)\n",
    "\n",
    "# Display the result\n",
    "print(\"Successfully computed embeddings for each text chunk.\")\n",
    "print(f\"Embeddings Shape: {document_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Step 6: Storing Document Embeddings in ChromaDB\n",
    "\n",
    "We initialize **ChromaDB**, a high-performance **vector database**, and store our computed embeddings to enable efficient retrieval of relevant text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Insert of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Insert of existing embedding ID: 3\n",
      "Add of existing embedding ID: 4\n",
      "Insert of existing embedding ID: 4\n",
      "Add of existing embedding ID: 5\n",
      "Insert of existing embedding ID: 5\n",
      "Add of existing embedding ID: 6\n",
      "Insert of existing embedding ID: 6\n",
      "Add of existing embedding ID: 7\n",
      "Insert of existing embedding ID: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully populated Chroma database with document embeddings.\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize and Populate the Chroma Vector Database ---\n",
    "\n",
    "# Define Chroma database path and collection name\n",
    "CHROMA_DB_PATH = \"./chroma_db\"\n",
    "COLLECTION_NAME = \"document_embeddings\"\n",
    "\n",
    "# Initialize Chroma client\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "\n",
    "# Add document embeddings to the Chroma collection\n",
    "for i, embedding in enumerate(document_embeddings):\n",
    "    collection.add(\n",
    "        ids=[str(i)],  # Chroma requires string IDs\n",
    "        embeddings=[embedding.tolist()],\n",
    "        metadatas=[{\"text\": doc_texts[i]}]\n",
    "    )\n",
    "\n",
    "print(\"Successfully populated Chroma database with document embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé Step 7: Implementing Vector Search Tool\n",
    "\n",
    "To retrieve relevant text passages from the database, we define a **vector search function** that finds the most relevant chunks based on a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the Vector Search Tool ---\n",
    "def vector_search_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches the Chroma database for relevant text chunks based on the query.\n",
    "    Computes the query embedding, retrieves the top 5 most relevant text chunks,\n",
    "    and returns them as a formatted string.\n",
    "    \"\"\"\n",
    "    # Compute the query embedding\n",
    "    query_embedding = embedding_model.encode(query, convert_to_numpy=True).tolist()\n",
    "    \n",
    "    # Define the number of nearest neighbors to retrieve\n",
    "    TOP_K = 5\n",
    "    \n",
    "    # Perform the search in the Chroma database\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=TOP_K\n",
    "    )\n",
    "    \n",
    "    # Retrieve and format the corresponding text chunks\n",
    "    retrieved_chunks = [metadata[\"text\"] for metadata in results[\"metadatas\"][0]]\n",
    "    return \"\\n\\n\".join(retrieved_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 8: Context Need Assessment\n",
    "\n",
    "Instead of always retrieving context, we determine if the query **requires external document context** before generating a response. This creates an agentic workflow that makes autonomous decisions to complete the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the Meta-Evaluation Function ---\n",
    "def needs_context(query: str) -> bool:\n",
    "    \"\"\"\n",
    "    Determines if additional context from an external document is required to generate an accurate and detailed answer.\n",
    "    Returns True if context is needed (response contains \"YES\"), False otherwise.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's query to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if external context is required, False otherwise.\n",
    "    \"\"\"\n",
    "    meta_prompt = (\n",
    "        \"Based on the following query, decide if additional context from an external document is needed \"\n",
    "        \"to generate an accurate and detailed answer. Have a tendency to use an external document if the query is not a very familiar topic. If in doubt, assume context is required and answer 'YES'.\\n\"\n",
    "        \"Answer with a single word: YES if additional context from an external document would be helpful to answer the query, \"\n",
    "        \"or NO if not. Do not say anything other than YES or NO.\\n\"\n",
    "        f\"Query: {query}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    meta_response = llm.invoke(meta_prompt)\n",
    "    print(\"Meta Response (is external document retrieval necessary?):\", meta_response)\n",
    "    return \"YES\" in meta_response.upper()\n",
    "\n",
    "\n",
    "# --- Define the Main Answer Generation Function with RAG (Retrieve and Generate) ---\n",
    "def generate_answer_with_agentic_rag(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a detailed and accurate answer to the user's query by using context when needed.\n",
    "    If additional context is required, it is retrieved from the vector store and included in the prompt.\n",
    "    If not, the answer is generated using the query alone.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's query to answer.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer based on the query.\n",
    "    \"\"\"\n",
    "    if needs_context(query):\n",
    "        # Retrieve additional context from the vector store\n",
    "        context = vector_search_tool(query)\n",
    "        \n",
    "        # Construct the enriched prompt with the additional context\n",
    "        enriched_prompt = (\n",
    "            \"Here is additional context from our document:\\n\"\n",
    "            f\"{context}\\n\\n\"\n",
    "            f\"Based on this context and the query: {query}\\n\"\n",
    "            \"Please provide a detailed and accurate answer.\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "        final_response = llm.invoke(enriched_prompt)\n",
    "    else:\n",
    "        # Generate an answer using the original query directly\n",
    "        direct_prompt = (\n",
    "            \"Please provide a detailed and accurate answer to the following query:\\n\"\n",
    "            f\"{query}\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "        final_response = llm.invoke(direct_prompt)\n",
    "    \n",
    "    return final_response\n",
    "\n",
    "\n",
    "# --- Define the Answer Generation Function without RAG ---\n",
    "def generate_answer_without_rag(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a detailed and accurate answer to the user's query without using any additional context from external documents.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's query to answer.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer based on the query.\n",
    "    \"\"\"\n",
    "    direct_prompt = (\n",
    "        \"Please provide a detailed and accurate answer to the following query:\\n\"\n",
    "        f\"{query}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    final_response = llm.invoke(direct_prompt)\n",
    "    \n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Step 9: Answer Generation with Agentic RAG\n",
    "\n",
    "If additional context is needed, the model retrieves **relevant document chunks** and incorporates them into the response prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: What are the key features of Z by HP AI Studio?\n",
      "Meta Response (is external document retrieval necessary?):  YES\n",
      "\n",
      "Final Answer:\n",
      " Based on the provided context, Z by HP AI Studio is a standalone application designed for data scientists and engineers that offers several key features to enhance their productivity and collaboration. Here are some of the key features of Z by HP AI Studio:\n",
      "1. Data Connectors: Z by HP AI Studio allows users to connect to multiple data-stores across local and cloud networks, making it easier to access the correct data and packages wherever they are.\n",
      "2. Local Computation: The platform enables users to perform all their computations locally without interruption, allowing them to manage development, data, and model environments without any disruptions.\n",
      "3. Monitoring: AI Studio runs the tools users select natively, providing real-time monitoring of GPU, CPU, and memory consumption. Users can visualize the effects of tests they run in real-time, giving them valuable insights into their project's performance.\n",
      "4. Notebooks: The platform provides a notebooks page where users can create and manage multiple notebooks simultaneously. Each tab on the notebooks page represents a different notebook, and users can have up to 5 running simultaneously.\n",
      "5. Data Fabric: Z by HP AI Studio offers a data fabric feature that allows users to mount their data directories in the container, enabling them to use the data to train machine learning models and run other high-velocity tests. Users can create new datasets, specify provider settings, and configure download and upload settings for their datasets.\n",
      "6. Reusing Workspace: The platform provides a feature that allows users to reuse workspaces, making it easier to start working on a project without having to set up everything from scratch. Users can view previously used workspaces, search for specific ones using the search bar, and name their workspace before starting a new project.\n",
      "7. Monitoring Page: The monitoring page provides users with valuable insights into their project's performance, including memory, cores, and GPU usage. Users can view the required resources for each workspace to run optimally, helping them determine which workstation best suits their needs.\n",
      "8. Generate PO: Z by HP AI Studio allows users to generate a purchase order (PO) for their projects, making it easier to manage billing and invoicing.\n",
      "9. Speak with an Expert: The platform provides an expert support team that users can speak with to get help with any issues or questions they may have.\n",
      "10. Company Details: The company details page serves as the admin home page, where admins can view and change the company name, billing email, and company logo. Admins can also use this page to delete their AI Studio account associated with their team and related projects.\n",
      "Overall, Z by HP AI Studio offers a range of features that enable data scientists and engineers to work more efficiently and collaboratively, making it an ideal platform for organizations looking to enhance their machine learning capabilities.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the key features of Z by HP AI Studio?\"\n",
    "print(\"User Query:\", query)\n",
    "final_answer = generate_answer_with_agentic_rag(query)\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Step 10: Answer Generation Without RAG\n",
    "\n",
    "In this case, we generate an answer without using RAG to show the difference between 2 answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: What are the key features of Z by HP AI Studio?\n",
      "\n",
      "Final Answer:\n",
      "\n",
      "Z by HP AI Studio is an all-in-one creative tool that enables users to design, edit, and share their digital content. Here are some of its key features:\n",
      "1. Design: Z by HP AI Studio offers a wide range of design tools, including templates, graphics, and text options. Users can create custom designs using these tools or use the AI-powered design assistant to generate unique designs based on their preferences.\n",
      "2. Editing: The platform provides advanced editing features, such as color correction, resizing, and cropping. Users can also add special effects, filters, and overlays to enhance their content.\n",
      "3. Collaboration: Z by HP AI Studio allows users to collaborate on designs in real-time. They can invite others to edit or view their designs, making it easier to work together on projects.\n",
      "4. Sharing: Once a design is complete, users can easily share it on social media platforms, messaging apps, or via email. The platform also supports the export of designs to popular formats like JPEG, PNG, and PDF.\n",
      "5. AI-Powered Design Assistant: Z by HP AI Studio includes an AI-powered design assistant that can help users generate unique designs based on their preferences. Users can provide input on the desired design elements, such as color scheme or theme, and the AI engine will create a custom design for them.\n",
      "6. Customizable Templates: The platform offers a range of customizable templates that users can use to create their designs. These templates are fully editable, allowing users to change colors, fonts, and other design elements to suit their needs.\n",
      "7. Integration with HP Printers: Z by HP AI Studio is designed to work seamlessly with HP printers, making it easy for users to print their designs directly from the platform. Users can select their preferred printer model and settings before printing, ensuring high-quality results every time.\n",
      "8. Mobile Compatibility: The platform is optimized for mobile devices, allowing users to access and edit their designs on-the-go. This makes it easy to work on projects anywhere, anytime.\n",
      "9. User-Friendly Interface: Z by HP AI Studio has an intuitive user interface that makes it easy for users of all skill levels to navigate. The platform is designed to be user-friendly, with clear and concise instructions provided throughout the design process.\n",
      "10. Access to a Community of Creatives: Z by HP AI Studio offers access to a community of creatives who can provide support, feedback, and inspiration. Users can connect with other designers, share their work, and learn from others in the community.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the key features of Z by HP AI Studio?\"\n",
    "print(\"User Query:\", query)\n",
    "final_answer = generate_answer_without_rag(query)\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
